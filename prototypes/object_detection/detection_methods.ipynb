{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Methods\n",
    "\n",
    "This code will consist of multiple methods to attempt to retrieve objects from images.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/matthijs/robotics_q3/mav/AE4317_2019_datasets/cyberzoo_aggressive_flight/20190121-144646/'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m             images\u001B[38;5;241m.\u001B[39mappend(img)\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m images\n\u001B[0;32m---> 17\u001B[0m images_cyberzoo \u001B[38;5;241m=\u001B[39m \u001B[43mload_images_from_folder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mIMAGES_FOLDER\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 11\u001B[0m, in \u001B[0;36mload_images_from_folder\u001B[0;34m(folder)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_images_from_folder\u001B[39m(folder):\n\u001B[1;32m     10\u001B[0m     images \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 11\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfolder\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     12\u001B[0m         img \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mimread(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(folder,filename))\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m img \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/matthijs/robotics_q3/mav/AE4317_2019_datasets/cyberzoo_aggressive_flight/20190121-144646/'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "IMAGES_FOLDER = \"/home/matthijs/robotics_q3/mav/AE4317_2019_datasets/cyberzoo_aggressive_flight/20190121-144646/\"\n",
    "VIDEO_FILE = \"/home/matthijs/robotics_q3/mav/cyberzoo.mp4\"\n",
    "VIDEO_FILE_SIM = \"/home/matthijs/paparazzi/videos/vlc-record-2023-03-03-11h21m26s-rtp_5000.sdp-.avi\"\n",
    "SHOW_CV2_IMG = False\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "images_cyberzoo = load_images_from_folder(IMAGES_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting opencv-python\r\n",
      "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.8/61.8 MB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17.0 in /home/sheharyar-ali/.local/lib/python3.8/site-packages (from opencv-python) (1.24.2)\r\n",
      "Installing collected packages: opencv-python\r\n",
      "Successfully installed opencv-python-4.7.0.72\r\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img, gray_scale=False, img_name=\"img\"):\n",
    "    if SHOW_CV2_IMG:\n",
    "        cv2.imshow(img_name, img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        if gray_scale:\n",
    "            cmap = \"gray\"\n",
    "        else:\n",
    "            cmap = None\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example image\n",
    "example_img = images_cyberzoo[0]\n",
    "img_name = \"cyberzoo_example\"\n",
    "show_img(example_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handcrafted edge-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to gray scale\n",
    "def rgb_2_gray(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images_cyberzoo[0]\n",
    "gray_img = rgb_2_gray(img)\n",
    "show_img(gray_img, gray_scale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_detection(filter, image):\n",
    "    gray_img = rgb_2_gray(image)\n",
    "    edges_img = np.zeros_like(gray_img)\n",
    "    N, M = gray_img.shape\n",
    "    for row in range(3, N-2):\n",
    "        for col in range(3,M-2):\n",
    "            local_pixels = gray_img[row-1:row+2, col-1:col+2]\n",
    "            transformed_pixels = filter * local_pixels\n",
    "            score = (transformed_pixels.sum() + 4)/8\n",
    "            edges_img[row,col] = score*3\n",
    "    return edges_img\n",
    "\n",
    "def canny_edge_detection(image, th1=100, th2=200):\n",
    "    edges = cv2.Canny(image, th1, th2)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_filter = np.array([[-1,-2,-1],[0,0,0],[1,2,1]])\n",
    "horizontal_filter = np.array([[-1,0,1],[-2,0,2],[-1,0,1]])\n",
    "\n",
    "canny_edges_img = canny_edge_detection(img)\n",
    "ver_edges_img = edge_detection(vertical_filter,img)\n",
    "hor_edges_img = edge_detection(horizontal_filter, img)\n",
    "plt.figure(1)\n",
    "plt.imshow(canny_edges_img, cmap=\"gray\")\n",
    "plt.figure(2)\n",
    "show_img(ver_edges_img, gray_scale=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corner Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images_cyberzoo[0]\n",
    "gray = rgb_2_gray(img)\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "#result is dilated for marking the corners, not important\n",
    "dst = cv2.dilate(dst,None)\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.01*dst.max()]=[0,0,255]\n",
    "show_img(img)\n",
    "if cv2.waitKey(0) & 0xff == ord('q'):\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leads to very useless data unfortunately"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT/SURF/ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = canny_edges_img # images_cyberzoo[0]\n",
    "orb = cv2.ORB_create()\n",
    "# surf = cv2.xfeatures2d.SURF_create()\n",
    "keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "\n",
    "img = cv2.drawKeypoints(img, keypoints, None)\n",
    "plt.imshow(img)\n",
    "if SHOW_CV2_IMG:\n",
    "    cv2.imshow(\"img\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def smoothen_image(image, size):\n",
    "    return cv2.blur(image, (size, size))\n",
    "\n",
    "def optical_flow(cap, first_frame, smoothen=0):\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Creates an image filled with zero\n",
    "    # intensities with the same dimensions \n",
    "    # as the frame\n",
    "    mask = np.zeros_like(first_frame)\n",
    "    \n",
    "    # Sets image saturation to maximum\n",
    "    mask[..., 1] = 255\n",
    "    \n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "        # ret = a boolean return value from getting\n",
    "        # the frame, frame = the current frame being\n",
    "        # projected in the video\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Opens a new window and displays the input\n",
    "        # frame\n",
    "        cv2.imshow(\"input\", frame)\n",
    "        \n",
    "        # Converts each frame to grayscale - we previously \n",
    "        # only converted the first frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Calculates dense optical flow by Farneback method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, \n",
    "                                        None,\n",
    "                                        0.5, 3, 45, 3, 5, 1.2, 0)\n",
    "        \n",
    "        # Computes the magnitude and angle of the 2D vectors\n",
    "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        \n",
    "        # Sets image hue according to the optical flow \n",
    "        # direction\n",
    "        mask[..., 0] = angle * 180 / np.pi / 2\n",
    "        \n",
    "        # Sets image value according to the optical flow\n",
    "        # magnitude (normalized)\n",
    "        mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        \n",
    "        # Converts HSV to RGB (BGR) color representation\n",
    "        rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        gray_flow_img = rgb_2_gray(rgb)\n",
    "        if smoothen > 0:\n",
    "            gray_flow_img = smoothen_image(gray_flow_img, smoothen)\n",
    "            #gray_flow_img = cv2.GaussianBlur(gray_flow_img, (9, 9),0)\n",
    "        # Opens a new window and displays the output frame\n",
    "        cv2.imshow(\"dense optical flow\", gray_flow_img)\n",
    "        cv2.imshow(\"dense optical flow\", rgb)\n",
    "        \n",
    "        # Updates previous frame\n",
    "        prev_gray = gray\n",
    "        \n",
    "        # Frames are read by intervals of 1 millisecond. The\n",
    "        # programs breaks out of the while loop when the\n",
    "        # user presses the 'q' key\n",
    "        if cv2.waitKey(33) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # The following frees up resources and\n",
    "    # closes all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(VIDEO_FILE_SIM)\n",
    "  \n",
    "# ret = a boolean return value from\n",
    "# getting the frame, first_frame = the\n",
    "# first frame in the entire video sequence\n",
    "ret, first_frame = cap.read()\n",
    "optical_flow(cap, first_frame, smoothen=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Optical flow implementation yields to way better results\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def draw_flow(img, flow, step=16):\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
    "    fx, fy = flow[y,x].T\n",
    "\n",
    "    lines = np.vstack([x, y, x-fx, y-fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.polylines(img_bgr, lines, 0, (0, 255, 0))\n",
    "\n",
    "    for (x1, y1), (_x2, _y2) in lines:\n",
    "        cv2.circle(img_bgr, (x1, y1), 1, (0, 255, 0), -1)\n",
    "\n",
    "    return img_bgr\n",
    "\n",
    "\n",
    "def draw_hsv(flow):\n",
    "\n",
    "    h, w = flow.shape[:2]\n",
    "    fx, fy = flow[:,:,0], flow[:,:,1]\n",
    "\n",
    "    ang = np.arctan2(fy, fx) + np.pi\n",
    "    v = np.sqrt(fx*fx+fy*fy)\n",
    "\n",
    "    hsv = np.zeros((h, w, 3), np.uint8)\n",
    "    hsv[...,0] = ang*(180/np.pi/2)\n",
    "    hsv[...,1] = 255\n",
    "    hsv[...,2] = np.minimum(v*4, 255)\n",
    "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return bgr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_FILE_SIM)\n",
    "\n",
    "suc, prev = cap.read()\n",
    "\n",
    "prevgray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "#prevgray = canny_edge_detection(prevgray)\n",
    "\n",
    "while True:\n",
    "\n",
    "    suc, img = cap.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #gray = canny_edge_detection(gray)\n",
    "    # start time to calculate FPS\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(prevgray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    prevgray = gray\n",
    "\n",
    "\n",
    "    # End time\n",
    "    end = time.time()\n",
    "    # calculate the FPS for current frame detection\n",
    "    fps = 1 / (end-start)\n",
    "\n",
    "    print(f\"{fps:.2f} FPS\")\n",
    "\n",
    "    cv2.imshow('flow', draw_flow(gray, flow))\n",
    "    cv2.imshow('flow HSV', draw_hsv(flow))\n",
    "\n",
    "\n",
    "    key = cv2.waitKey(5)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed methods:\n",
    "  1.  To extract objects from the noisy data: perform DBScan clustering twice. First on the color to get everything that has been appointed a similar color/similar depth with the optical flow and then a second round of clustering is performed on these Clusters. The problem with this approach is that you need to fit the second level clusters for each iteration. So this is not a feasible solution :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
