{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Methods\n",
    "\n",
    "This code will consist of multiple methods to attempt to retrieve objects from images.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from match_image_data import convert_csv_2_dict, load_images_from_folder, get_drone_state\n",
    "\n",
    "DATAFILE_DRONE = \"/home/matthijs/paparazzi/data/AE4317_2019_datasets/cyberzoo_aggressive_flight/20190121-144708.csv\"\n",
    "IMAGES_FOLDER = \"/home/matthijs/paparazzi/data/AE4317_2019_datasets/cyberzoo_aggressive_flight/20190121-144646/\"\n",
    "VIDEO_FILE = \"/home/matthijs/robotics_q3/mav/cyberzoo.mp4\"\n",
    "VIDEO_FILE_SIM = \"/home/matthijs/paparazzi/videos/vlc-record-2023-03-03-11h21m26s-rtp_5000.sdp-.avi\"\n",
    "SHOW_CV2_IMG = False\n",
    "SHOW_VIDEO = False\n",
    "\n",
    "drone_state_dict = convert_csv_2_dict(DATAFILE_DRONE)\n",
    "images_cyberzoo_dict = load_images_from_folder(IMAGES_FOLDER)\n",
    "images_cyberzoo = images_cyberzoo_dict[\"images\"]\n",
    "images_cyberzoo_stamp = images_cyberzoo_dict[\"timestamps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img, gray_scale=False, img_name=\"img\"):\n",
    "    if SHOW_CV2_IMG:\n",
    "        cv2.imshow(img_name, img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        if gray_scale:\n",
    "            cmap = \"gray\"\n",
    "        else:\n",
    "            cmap = None\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example image\n",
    "example_img = images_cyberzoo[200]\n",
    "img_name = \"cyberzoo_example\"\n",
    "show_img(example_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handcrafted edge-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to gray scale\n",
    "def rgb_2_gray(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images_cyberzoo[200]\n",
    "gray_img = rgb_2_gray(img)\n",
    "show_img(gray_img, gray_scale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_detection(filter, image):\n",
    "    gray_img = rgb_2_gray(image)\n",
    "    edges_img = np.zeros_like(gray_img)\n",
    "    N, M = gray_img.shape\n",
    "    for row in range(3, N-2):\n",
    "        for col in range(3,M-2):\n",
    "            local_pixels = gray_img[row-1:row+2, col-1:col+2]\n",
    "            transformed_pixels = filter * local_pixels\n",
    "            score = (transformed_pixels.sum() + 4)/8\n",
    "            edges_img[row,col] = score*3\n",
    "    return edges_img\n",
    "\n",
    "def canny_edge_detection(image, th1=100, th2=200):\n",
    "    edges = cv2.Canny(image, th1, th2)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_filter = np.array([[-1,-2,-1],[0,0,0],[1,2,1]])\n",
    "horizontal_filter = np.array([[-1,0,1],[-2,0,2],[-1,0,1]])\n",
    "\n",
    "canny_edges_img = canny_edge_detection(img)\n",
    "ver_edges_img = edge_detection(vertical_filter,img)\n",
    "hor_edges_img = edge_detection(horizontal_filter, img)\n",
    "plt.figure(1)\n",
    "plt.imshow(canny_edges_img, cmap=\"gray\")\n",
    "plt.figure(2)\n",
    "show_img(ver_edges_img, gray_scale=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corner Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.copy(images_cyberzoo[200])\n",
    "gray = rgb_2_gray(img)\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "#result is dilated for marking the corners, not important\n",
    "dst = cv2.dilate(dst,None)\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.01*dst.max()]=[0,0,255]\n",
    "show_img(img)\n",
    "if cv2.waitKey(0) & 0xff == ord('q'):\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leads to very useless data unfortunately"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT/SURF/ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = canny_edges_img # images_cyberzoo[200]\n",
    "orb = cv2.ORB_create()\n",
    "# surf = cv2.xfeatures2d.SURF_create()\n",
    "keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "\n",
    "img = cv2.drawKeypoints(img, keypoints, None)\n",
    "plt.imshow(img)\n",
    "if SHOW_CV2_IMG:\n",
    "    cv2.imshow(\"img\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def smoothen_image(image, size):\n",
    "    return cv2.blur(image, (size, size))\n",
    "\n",
    "def optical_flow(cap, first_frame, smoothen=0):\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Creates an image filled with zero\n",
    "    # intensities with the same dimensions \n",
    "    # as the frame\n",
    "    mask = np.zeros_like(first_frame)\n",
    "    \n",
    "    # Sets image saturation to maximum\n",
    "    mask[..., 1] = 255\n",
    "    \n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "        # ret = a boolean return value from getting\n",
    "        # the frame, frame = the current frame being\n",
    "        # projected in the video\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Opens a new window and displays the input\n",
    "        # frame\n",
    "        cv2.imshow(\"input\", frame)\n",
    "        \n",
    "        # Converts each frame to grayscale - we previously \n",
    "        # only converted the first frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Calculates dense optical flow by Farneback method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, \n",
    "                                        None,\n",
    "                                        0.5, 3, 45, 3, 5, 1.2, 0)\n",
    "        \n",
    "        # Computes the magnitude and angle of the 2D vectors\n",
    "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        \n",
    "        # Sets image hue according to the optical flow \n",
    "        # direction\n",
    "        mask[..., 0] = angle * 180 / np.pi / 2\n",
    "        \n",
    "        # Sets image value according to the optical flow\n",
    "        # magnitude (normalized)\n",
    "        mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        \n",
    "        # Converts HSV to RGB (BGR) color representation\n",
    "        rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        gray_flow_img = rgb_2_gray(rgb)\n",
    "        if smoothen > 0:\n",
    "            gray_flow_img = smoothen_image(gray_flow_img, smoothen)\n",
    "            #gray_flow_img = cv2.GaussianBlur(gray_flow_img, (9, 9),0)\n",
    "        # Opens a new window and displays the output frame\n",
    "        cv2.imshow(\"dense optical flow\", gray_flow_img)\n",
    "        cv2.imshow(\"dense optical flow\", rgb)\n",
    "        \n",
    "        # Updates previous frame\n",
    "        prev_gray = gray\n",
    "        \n",
    "        # Frames are read by intervals of 1 millisecond. The\n",
    "        # programs breaks out of the while loop when the\n",
    "        # user presses the 'q' key\n",
    "        if cv2.waitKey(33) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # The following frees up resources and\n",
    "    # closes all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_VIDEO:\n",
    "    cap = cv2.VideoCapture(VIDEO_FILE_SIM)\n",
    "    \n",
    "    # ret = a boolean return value from\n",
    "    # getting the frame, first_frame = the\n",
    "    # first frame in the entire video sequence\n",
    "    ret, first_frame = cap.read()\n",
    "    optical_flow(cap, first_frame, smoothen=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Optical flow implementation yields to way better results\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def draw_flow(img, flow, step=16):\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
    "    fx, fy = flow[y,x].T\n",
    "\n",
    "    lines = np.vstack([x, y, x-fx, y-fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.polylines(img_bgr, lines, 0, (0, 255, 0))\n",
    "\n",
    "    for (x1, y1), (_x2, _y2) in lines:\n",
    "        cv2.circle(img_bgr, (x1, y1), 1, (0, 255, 0), -1)\n",
    "\n",
    "    return img_bgr\n",
    "\n",
    "\n",
    "def draw_hsv(flow):\n",
    "\n",
    "    h, w = flow.shape[:2]\n",
    "    fx, fy = flow[:,:,0], flow[:,:,1]\n",
    "\n",
    "    ang = np.arctan2(fy, fx) + np.pi\n",
    "    v = np.sqrt(fx*fx+fy*fy)\n",
    "\n",
    "    hsv = np.zeros((h, w, 3), np.uint8)\n",
    "    hsv[...,0] = ang*(180/np.pi/2)\n",
    "    hsv[...,1] = 255\n",
    "    hsv[...,2] = np.minimum(v*4, 255)\n",
    "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return bgr\n",
    "\n",
    "\n",
    "\n",
    "if SHOW_VIDEO:\n",
    "    cap = cv2.VideoCapture(VIDEO_FILE_SIM)\n",
    "\n",
    "    suc, prev = cap.read()\n",
    "\n",
    "    prevgray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "    #prevgray = canny_edge_detection(prevgray)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        suc, img = cap.read()\n",
    "        \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        #gray = canny_edge_detection(gray)\n",
    "        # start time to calculate FPS\n",
    "        start = time.time()\n",
    "\n",
    "\n",
    "        flow = cv2.calcOpticalFlowFarneback(prevgray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        \n",
    "        prevgray = gray\n",
    "\n",
    "\n",
    "        # End time\n",
    "        end = time.time()\n",
    "        # calculate the FPS for current frame detection\n",
    "        fps = 1 / (end-start)\n",
    "\n",
    "        print(f\"{fps:.2f} FPS\")\n",
    "\n",
    "        cv2.imshow('flow', draw_flow(gray, flow))\n",
    "        cv2.imshow('flow HSV', draw_hsv(flow))\n",
    "\n",
    "\n",
    "        key = cv2.waitKey(5)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed methods:\n",
    "  1.  To extract objects from the noisy data: perform DBScan clustering twice. First on the color to get everything that has been appointed a similar color/similar depth with the optical flow and then a second round of clustering is performed on these Clusters. The problem with this approach is that you need to fit the second level clusters for each iteration. So this is not a feasible solution :("
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cv2 as cv\n",
    "\n",
    "# backSub = cv.createBackgroundSubtractorMOG2()\n",
    "if SHOW_VIDEO:\n",
    "    backSub = cv.createBackgroundSubtractorKNN()\n",
    "    capture = cv.VideoCapture(cv.samples.findFileOrKeep(VIDEO_FILE_SIM))\n",
    "    if not capture.isOpened():\n",
    "        print('Unable to open: ' + VIDEO_FILE_SIM)\n",
    "        exit(0)\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "        if frame is None:\n",
    "            break\n",
    "        \n",
    "        fgMask = backSub.apply(frame)\n",
    "        \n",
    "        \n",
    "        cv.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n",
    "        cv.putText(frame, str(capture.get(cv.CAP_PROP_POS_FRAMES)), (15, 15),\n",
    "                cv.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n",
    "        \n",
    "        \n",
    "        cv.imshow('Frame', frame)\n",
    "        cv.imshow('FG Mask', fgMask)\n",
    "        \n",
    "        keyboard = cv.waitKey(30)\n",
    "        if keyboard == 'q' or keyboard == 27:\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gray Scale\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster = KMeans(n_clusters=3)\n",
    "img = rgb_2_gray(images_cyberzoo[200])\n",
    "X = img.reshape(-1,1)\n",
    "kmeans = cluster.fit(X)\n",
    "\n",
    "lab = kmeans.labels_\n",
    "img_clusters = 255/(lab.max() - lab.min()) * lab\n",
    "img_clusters = img_clusters.reshape(img.shape)\n",
    "plt.imshow(img_clusters, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_color_cluster(img, cluster):\n",
    "    img_reshape = img.reshape(img.shape[0]*img.shape[1], img.shape[2])\n",
    "    lab_color = cluster.predict(img_reshape)\n",
    "    lab_color = lab_color.reshape(img.shape[0], img.shape[1])\n",
    "    img_clustered = np.copy(img)\n",
    "    for i in np.unique(lab_color):\n",
    "        mean_color = np.mean(img[lab_color==i].T, axis=1)\n",
    "        print(mean_color)\n",
    "        img_clustered[lab_color==i] = mean_color\n",
    "    return img_clustered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color Scale\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "cluster_color = KMeans(n_clusters=4) #DBSCAN(eps=20, min_samples=100)\n",
    "#cluster_color = DBSCAN(eps=5, min_samples=1000)\n",
    "\n",
    "img = np.copy(images_cyberzoo[200])\n",
    "img_reshape = img.reshape(img.shape[0]*img.shape[1], img.shape[2])\n",
    "dbscan = cluster_color.fit(img_reshape)\n",
    "img_clustered = perform_color_cluster(img, cluster_color)\n",
    "# lab_color = dbscan.labels_\n",
    "# lab_color = lab_color.reshape(img.shape[0], img.shape[1])\n",
    "# img_clustered = np.copy(img)\n",
    "# for i in np.unique(lab_color):\n",
    "#     mean_color = np.mean(img[lab_color==i].T, axis=1)\n",
    "#     img_clustered[lab_color==i] = mean_color\n",
    "\n",
    "show_img(img_clustered)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cyberzoo[0]` gives quite a nice overview, so maybe we can use the clusterer trained on this set to predict on other sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.copy(images_cyberzoo[100])\n",
    "clustered_test_image = perform_color_cluster(test_image, cluster_color)\n",
    "print(clustered_test_image)\n",
    "show_img(clustered_test_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes quite a good extinction between the ground, the pilars and the background so this could be quite useful"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annoying thing is that KMeans cant have variable boundaries and you cant predefine a boundary. So DBScan would be nice to use. However The data is a bit too dense to do this rn, so pooling could help allow the use of DBScan over KMeans. Maybe sth else than max pooling, since there are 3 channels with rgb images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# pooling_layer = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "\n",
    "def scale_down_img(img, factor=5):\n",
    "    assert (img.shape[0]%factor==0 and img.shape[1]%factor==0), \"not feasible scaling factor\"\n",
    "    scaled_img = np.zeros((img.shape[0]//factor, img.shape[1]//factor, img.shape[2]))\n",
    "    for i in range(scaled_img.shape[0]):\n",
    "        for j in range(scaled_img.shape[1]):\n",
    "            scaled_img[i, j] = img[i*factor, j*factor]\n",
    "    return scaled_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.copy(images_cyberzoo[200])\n",
    "scaled_img = scale_down_img(img)\n",
    "show_img(img)\n",
    "\n",
    "cluster_color = DBSCAN(eps=7, min_samples=50)\n",
    "\n",
    "img_reshape = scaled_img.reshape(scaled_img.shape[0]*scaled_img.shape[1], scaled_img.shape[2])\n",
    "lab_colors = cluster_color.fit_predict(img_reshape)\n",
    "\n",
    "img_clustered = np.copy(scaled_img)\n",
    "lab_colors = lab_colors.reshape(scaled_img.shape[0], scaled_img.shape[1])\n",
    "for i in np.unique(lab_colors):\n",
    "    if i==-1:\n",
    "        print(np.unique(lab_colors))\n",
    "        img_clustered[lab_colors==i] = np.array([0,0,0])\n",
    "        continue\n",
    "    mean_color = np.mean(scaled_img[lab_colors==i].T, axis=1)/255\n",
    "    img_clustered[lab_colors==i] = mean_color\n",
    "\n",
    "show_img(img_clustered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this we do need to identify which classes are the obstacles\n",
    "OBS_CLASS = 1 # we still need a way to identify which classes represent obstacles\n",
    "img_clustered_copy = np.copy(img_clustered)\n",
    "\n",
    "obs_locations = np.argwhere(lab_colors==OBS_CLASS)\n",
    "print(obs_locations[0,0])\n",
    "print(img_clustered_copy[obs_locations[:,0], obs_locations[:,1]])\n",
    "cluster_position = DBSCAN(eps=10, min_samples=5)\n",
    "position_groups = cluster_position.fit_predict(obs_locations)\n",
    "#print(obs_locations[position_groups==0])\n",
    "for i in np.unique(position_groups):\n",
    "    if i==-1:\n",
    "        continue\n",
    "    img_clustered_copy[obs_locations[position_groups==i][:,0], obs_locations[position_groups==i][:,1]] = np.random.random(3)\n",
    "show_img(img_clustered_copy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img_clustered\n",
    "phi = get_drone_state(drone_state_dict, images_cyberzoo_stamp[200])['att_phi']\n",
    "\n",
    "print(img.shape[0])\n",
    "for i in np.linspace(0, img.shape[0] + 20, 40):\n",
    "    x = np.linspace(0, img.shape[1], 100)\n",
    "    y = -np.sin(phi) * x + i\n",
    "    show_img(img_clustered)\n",
    "    plt.xlim(0, img.shape[1])\n",
    "    plt.ylim(img.shape[0], 0)\n",
    "\n",
    "    plt.plot(x, y, color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
